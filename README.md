# SLA
This repository will provide the official implementation of SLA (Sparse–Linear Attention).

SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse–Linear Attention  
Paper: https://www.arxiv.org/pdf/2509.24006  
Jintao Zhang, Haoxu Wang, Kai Jiang, Shuo Yang, Kaiwen Zheng, Haocheng Xi, Ziteng Wang, Hongzhou Zhu, Min Zhao, Ion Stoica, Joseph E. Gonzalez, Jun Zhu, Jianfei Chen

![SLA Overview](./assets/overview_of_SLA.png)

### Motivation
![SLA Motivation](./assets/SLA_motivation.png)

### Effectiveness
![SLA Effectiveness](./assets/SLA_effectiveness.png)

### Efficiency
![SLA Efficiency](./assets/SLA_efficiency.png)

## Code Release Progress
We are still optimizing the code, and will release it to this repository ASAP.  
We plan to open-source the code within two weeks, but please note that this is not a promise.



## Citation

If you find this work useful in your research, please cite:

```bibtex
@article{zhang2025sla,
  title={SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse–Linear Attention},
  author={Zhang, Jintao and Wang, Haoxu and Jiang, Kai and Yang, Shuo and Zheng, Kaiwen and Xi, Haocheng and Wang, Ziteng and Zhu, Hongzhou and Zhao, Min and Stoica, Ion and Gonzalez, Joseph E. and Zhu, Jun and Chen, Jianfei},
  journal={arXiv preprint arXiv:2509.24006},
  year={2025}
}
```
